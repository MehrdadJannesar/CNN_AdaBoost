{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jannesar_Shooshtari_Laitec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJannesar/CNN_AdaBoost/blob/master/Jannesar_Shooshtari_Laitec_Improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhgLZFSI6XvY",
        "colab_type": "code",
        "outputId": "2bcba2bd-aadc-45ed-ba48-c6e62985a127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets(\"../1-dnn/\", one_hot=True)\n",
        "\n",
        "# Training Parameters\n",
        "num_steps = 200\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "strides = 1\n",
        "k = 2\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784  # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.5  # Dropout, probability to keep unit\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder('float', [None, num_input])\n",
        "Y = tf.placeholder('float',shape=[None,num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-af1da49a26b6>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../1-dnn/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../1-dnn/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ../1-dnn/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ../1-dnn/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhabes8A8UUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_cnn(x,dropout,num_classes):\n",
        "  x = tf.reshape(x, shape=[-1,28,28,1])\n",
        "\n",
        " \n",
        "  w_c_1 = tf.Variable(tf.random_normal([3, 3, 1, 32]))\n",
        "  w_c_2 = tf.Variable(tf.random_normal([3, 3, 32, 64]))\n",
        "  w_c_3 = tf.Variable(tf.random_normal([3, 3, 64, 128]))\n",
        "  b_c_1 = tf.Variable(tf.zeros([32]))\n",
        "  b_c_2 = tf.Variable(tf.zeros([64]))\n",
        "  b_c_3 = tf.Variable(tf.zeros([128]))\n",
        "  # The second three convolutional layer weights\n",
        "  w_c_4 = tf.Variable(tf.random_normal([3, 3, 128, 256]))\n",
        "  w_c_5 = tf.Variable(tf.random_normal([3, 3, 256, 512]))\n",
        "  w_c_6 = tf.Variable(tf.random_normal([3, 3, 512, 1024]))\n",
        "  b_c_4 = tf.Variable(tf.zeros([256]))\n",
        "  b_c_5 = tf.Variable(tf.zeros([512]))\n",
        "  b_c_6 = tf.Variable(tf.zeros([1024]))\n",
        "  # Fully connected weight\n",
        "  w_f_1 = tf.Variable(tf.random_normal([7*7*1024, 2048]))\n",
        "  w_f_2 = tf.Variable(tf.random_normal([2048, 1024]))\n",
        "  w_f_3 = tf.Variable(tf.random_normal([1024, 512]))\n",
        "  b_f_1 = tf.Variable(tf.zeros([2048]))\n",
        "  b_f_2 = tf.Variable(tf.zeros([1024]))\n",
        "  b_f_3 = tf.Variable(tf.zeros([512]))\n",
        "  \n",
        "  w_out = tf.Variable(tf.random_normal([512, num_classes]))\n",
        "  b_out = tf.Variable(tf.zeros([num_classes]))\n",
        "  \n",
        "  # first layer convolution\n",
        "  conv1 = tf.nn.bias_add(tf.nn.conv2d(x,w_c_1,name='C1',strides=[1,1,1,1],padding='SAME'),b_c_1)\n",
        "  conv1 = tf.nn.relu(conv1)\n",
        "  \n",
        "  # second layer convolution\n",
        "  conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1,w_c_2,name='C2',strides=[1,1,1,1],padding='SAME'),b_c_2)\n",
        "  conv2 = tf.nn.relu(conv2)\n",
        "  \n",
        "  # third layer convolution\n",
        "  conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2,w_c_3,name='C3',strides=[1,1,1,1],padding='SAME'),b_c_3)\n",
        "  conv3 = tf.nn.relu(conv3)\n",
        "  \n",
        "  # first Max Pooling (down-sampling)\n",
        "  pool_1 = tf.nn.max_pool2d(conv3,name='p1', ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
        "  \n",
        "  # fourth layer convolution\n",
        "  conv4 = tf.nn.bias_add(tf.nn.conv2d(pool_1,w_c_4,name='C4',strides=[1,1,1,1],padding='SAME'),b_c_4)\n",
        "  conv4 = tf.nn.relu(conv4)\n",
        "  \n",
        "  # fifth layer convolution\n",
        "  conv5 = tf.nn.bias_add(tf.nn.conv2d(conv4,w_c_5,name='C5',strides=[1,1,1,1],padding='SAME'),b_c_5)\n",
        "  conv5 = tf.nn.relu(conv5)\n",
        "  \n",
        "  # sixth layer convolution\n",
        "  conv6 = tf.nn.bias_add(tf.nn.conv2d(conv5,w_c_6,name='C6',strides=[1,1,1,1],padding='SAME'),b_c_6)\n",
        "  conv6 = tf.nn.relu(conv6)\n",
        "  \n",
        "  # second Max Pooling (down-sampling)\n",
        "  pool_2 = tf.nn.max_pool2d(conv6,name='p2',ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
        "  \n",
        "  # first Fully connected layer\n",
        "  # Reshape conv6 output to fit fully connected layer input\n",
        "  # fc1 = tf.layers.dense(pool_2, 1024)\n",
        "  fc1 = tf.reshape(pool_2, [-1, w_f_1.get_shape().as_list()[0]])\n",
        "  fc1 = tf.add(tf.matmul(fc1, w_f_1), b_f_1)\n",
        "  fc1 = tf.nn.relu(fc1)\n",
        "  # Apply Dropout\n",
        "  fc1 = tf.nn.dropout(fc1, dropout)\n",
        "  \n",
        "  # second Fully connected layer\n",
        "  # fc2 = tf.layers.dense(fc1, 1024)\n",
        "  fc2 = tf.reshape(fc1, [-1, w_f_2.get_shape().as_list()[0]])\n",
        "  fc2 = tf.add(tf.matmul(fc2, w_f_2), b_f_2)\n",
        "  fc2 = tf.nn.relu(fc2)\n",
        "  # Third Fully connected layer\n",
        "  # fc3 = tf.layers.dense(fc2, 1024)\n",
        "  fc3 = tf.reshape(fc2, [-1, w_f_3.get_shape().as_list()[0]])\n",
        "  fc3 = tf.add(tf.matmul(fc3, w_f_3), b_f_3)\n",
        "  fc3 = tf.nn.relu(fc3)\n",
        "\n",
        "  out = tf.add(tf.matmul(fc3, w_out), b_out)\n",
        "  return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiXVAz7_EJp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "94e0d9e5-cd56-4598-aa80-d7604790fee4"
      },
      "source": [
        "logits = model_cnn(X, keep_prob,num_classes)\n",
        "prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-2f978818b8b7>:66: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCXAtJXEc1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bc0cc1fa-08f5-46e1-e75a-7814544c8305"
      },
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "train_op = optimizer.minimize(loss_op)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-d003734c7149>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERUB2zzEkna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDf-TpOW6mF2",
        "colab_type": "code",
        "outputId": "801be2f5-d5fb-4036-cdb8-12a68c29c8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Please don't change these.\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # batch_x = np.reshape(batch_x, (-1, 28, 28, 1))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 256 MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
        "                                      Y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.0}))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 20962839887872.0000, Training Accuracy= 0.195\n",
            "Step 10, Minibatch Loss= 3457039204352.0000, Training Accuracy= 0.609\n",
            "Step 20, Minibatch Loss= 1037471842304.0000, Training Accuracy= 0.797\n",
            "Step 30, Minibatch Loss= 452625268736.0000, Training Accuracy= 0.898\n",
            "Step 40, Minibatch Loss= 353807302656.0000, Training Accuracy= 0.898\n",
            "Step 50, Minibatch Loss= 366857486336.0000, Training Accuracy= 0.914\n",
            "Step 60, Minibatch Loss= 136697970688.0000, Training Accuracy= 0.938\n",
            "Step 70, Minibatch Loss= 338091114496.0000, Training Accuracy= 0.906\n",
            "Step 80, Minibatch Loss= 296592769024.0000, Training Accuracy= 0.906\n",
            "Step 90, Minibatch Loss= 182943154176.0000, Training Accuracy= 0.914\n",
            "Step 100, Minibatch Loss= 234992566272.0000, Training Accuracy= 0.922\n",
            "Step 110, Minibatch Loss= 96217079808.0000, Training Accuracy= 0.938\n",
            "Step 120, Minibatch Loss= 87716659200.0000, Training Accuracy= 0.945\n",
            "Step 130, Minibatch Loss= 110613250048.0000, Training Accuracy= 0.969\n",
            "Step 140, Minibatch Loss= 182054551552.0000, Training Accuracy= 0.945\n",
            "Step 150, Minibatch Loss= 166353600512.0000, Training Accuracy= 0.938\n",
            "Step 160, Minibatch Loss= 248530255872.0000, Training Accuracy= 0.938\n",
            "Step 170, Minibatch Loss= 157932584960.0000, Training Accuracy= 0.930\n",
            "Step 180, Minibatch Loss= 96698040320.0000, Training Accuracy= 0.953\n",
            "Step 190, Minibatch Loss= 5770903552.0000, Training Accuracy= 0.984\n",
            "Step 200, Minibatch Loss= 104411824128.0000, Training Accuracy= 0.953\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.96484375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSdD1-ivR9QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}