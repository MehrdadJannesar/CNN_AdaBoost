{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet101.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJannesar/CNN_AdaBoost/blob/master/ResNet101_laitec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UczYE_bvSKYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "2928b20f-a56d-4fe3-8ed9-5798161147b0"
      },
      "source": [
        "#Import libraries\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,Input\n",
        "from keras.applications.resnet import ResNet101\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwAl97hmSug2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8a77e04e-465e-4aa9-ed7f-2085fa0040fb"
      },
      "source": [
        "#Load Dataset\n",
        "#Optimize and categorize dataset\n",
        "(train_images,train_labels),(test_images,test_labels) = cifar10.load_data()\n",
        "\n",
        "X_train = train_images.astype('float32')\n",
        "X_test = test_images.astype('float32')\n",
        "\n",
        "X_train /=255\n",
        "X_test /=255\n",
        "N_classes = 10\n",
        "Y_train = keras.utils.to_categorical(train_labels,N_classes)\n",
        "Y_test = keras.utils.to_categorical(test_labels,N_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNHRBbW1Uop8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "7ac1a8c2-3722-4f09-a31d-c6f5484587d8"
      },
      "source": [
        "#Create model\n",
        "\"\"\"\n",
        "---> input layer \n",
        "---> close Resnet50 layers\n",
        "---> fullyConnect 10 Classes\n",
        "\n",
        "I don't open ResNet layer in this code, But I explained it in previous code (ResNet50_laitec)\n",
        "\"\"\"\n",
        "input_layer = Input(shape=(32,32,3))\n",
        "resnet = ResNet101(include_top = False, pooling = 'avg',weights=None)(input_layer)\n",
        "dense = Dense(10,activation='softmax')(resnet)\n",
        "\n",
        "mymodel = Model(input_layer, dense)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAjl6t4SVJ6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3964986d-2fda-46ab-c65d-c08c42c8724a"
      },
      "source": [
        "#Compile my model and Defining optimizers\n",
        "#Optimizer is SGD (Stochastic Gradient Descent) and loss function is Crossentropy\n",
        "sgd = keras.optimizers.SGD(lr=0.01, decay = 1e-6,momentum=0.9,nesterov=True)\n",
        "mymodel.compile(optimizer=sgd ,loss = keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48oQIJ0AVp1N",
        "colab_type": "code",
        "outputId": "5422e705-e792-4710-919f-96d764bcdf7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "#Fit myModel\n",
        "net = mymodel.fit(X_train,Y_train, batch_size=128,epochs=10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 61s 1ms/step - loss: 0.5366 - acc: 0.8347\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 61s 1ms/step - loss: 0.5030 - acc: 0.8462\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 62s 1ms/step - loss: 0.4799 - acc: 0.8572\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 61s 1ms/step - loss: 0.5689 - acc: 0.8328\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 62s 1ms/step - loss: 0.4773 - acc: 0.8635\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 61s 1ms/step - loss: 0.4424 - acc: 0.8746\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 62s 1ms/step - loss: 0.3986 - acc: 0.8895\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 63s 1ms/step - loss: 0.3675 - acc: 0.9001\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 62s 1ms/step - loss: 0.3494 - acc: 0.9071\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 62s 1ms/step - loss: 0.3338 - acc: 0.9131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_jgEJFdtsv5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6f5f022e-8205-4c5c-b240-b5f17d5cd11d"
      },
      "source": [
        "#Predict network\n",
        "#Validation myModel (Loss,Accurcy)\n",
        "test_loss, test_acc = mymodel.evaluate(X_test, Y_test)\n",
        "test_prediction = mymodel.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"test_loss: \",test_loss)\n",
        "print(\"test_acc: \",test_acc)\n",
        "print(\"test_prediction\", test_prediction)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 9s 907us/step\n",
            "test_loss:  2.1956620325088503\n",
            "test_acc:  0.5551\n",
            "test_prediction [[7.8654237e-05 6.0090120e-04 5.6901299e-03 ... 7.6884913e-05\n",
            "  1.9280907e-03 1.3088972e-03]\n",
            " [8.8981236e-05 8.2478749e-05 5.2503552e-08 ... 1.3602178e-12\n",
            "  9.9534494e-01 4.4835052e-03]\n",
            " [5.4917741e-04 9.8002481e-04 1.3027161e-06 ... 6.5098597e-08\n",
            "  9.5274866e-01 4.5719810e-02]\n",
            " ...\n",
            " [1.8268431e-02 4.8153140e-03 1.7244960e-01 ... 7.4495472e-02\n",
            "  1.1325207e-03 2.1502201e-03]\n",
            " [3.2949891e-05 1.0229479e-02 8.2259365e-05 ... 3.2498199e-02\n",
            "  2.2982243e-03 2.7225579e-03]\n",
            " [9.0687972e-06 1.8935613e-13 4.0436455e-05 ... 9.9994671e-01\n",
            "  2.9837344e-12 1.1680923e-09]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHevYVF6xIPA",
        "colab_type": "text"
      },
      "source": [
        "ResNet\n",
        "Deep residual networks were a breakthrough idea which enabled the development of much deeper networks (hundreds of layers as opposed to tens of layers).\n",
        "\n",
        "Its a generally accepted principle that deeper networks are capable of learning more complex functions and representations of the input which should lead to better performance. However, many researchers observed that adding more layers eventually had a negative effect on the final performance. This behavior was not intuitively expected, as explained by the authors below.\n",
        "\n",
        "Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).\n",
        "\n",
        "This phenomenon is referred to by the authors as the degradation problem - alluding to the fact that although better parameter initialization techniques and batch normalization allow for deeper networks to converge, they often converge at a higher error rate than their shallower counterparts. In the limit, simply stacking more layers degrades the model's ultimate performance.\n",
        "\n",
        "The authors propose a remedy to this degradation problem by introducing residual blocks in which intermediate layers of a block learn a residual function with reference to the block input. You can think of this residual function as a refinement step in which we learn how to adjust the input feature map for higher quality features. This compares with a \"plain\" network in which each layer is expected to learn new and distinct feature maps. In the event that no refinement is needed, the intermediate layers can learn to gradually adjust their weights toward zero such that the residual block represents an identity function.\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-16-at-6.29.19-PM.png)\n",
        "\n",
        "Note: It was later discovered that a slight modification to the original proposed unit offers better performance by more efficiently allowing gradients to propagate through the network during training.\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-17-at-10.36.21-PM.png)\n",
        "\n",
        "Wide residual networks\n",
        "Although the original ResNet paper focused on creating a network architecture to enable deeper structures by alleviating the degradation problem, other researchers have since pointed out that increasing the network's width (channel depth) can be a more efficient way of expanding the overall capacity of the network.\n",
        "\n",
        "Architecture\n",
        "\n",
        "![](https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-16-at-6.30.05-PM.png)\n",
        "\n",
        "Each colored block of layers represent a series of convolutions of the same dimension. The feature mapping is periodically downsampled by strided convolution accompanied by an increase in channel depth to preserve the time complexity per layer. Dotted lines denote residual connections in which we project the input via a 1x1 convolution to match the dimensions of the new block.\n",
        "\n",
        "The diagram above visualizes the ResNet 34 architecture. For the ResNet 50 model, we simply replace each two layer residual block with a three layer bottleneck block which uses 1x1 convolutions to reduce and subsequently restore the channel depth, allowing for a reduced computational load when calculating the 3x3 convolution.\n",
        "\n",
        "![alt text](https://www.jeremyjordan.me/content/images/2018/04/Screen-Shot-2018-04-16-at-5.47.38-PM.png)\n",
        "\n",
        "Parameters: 25 million (ResNet 50)\n",
        "\n",
        "Papers:\n",
        "\n",
        "Deep Residual Learning for Image Recognition\n",
        "Identity Mappings in Deep Residual Networks\n",
        "Wide Residual Networks\n",
        "\n",
        "ResNet-101:\n",
        "\n",
        "![alt text](https://resources.wolframcloud.com/NeuralNetRepository/resources/images/853/8536f93a-af9a-4f71-a516-87eee51cc193-io-1-o.en.gif)"
      ]
    }
  ]
}