{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJannesar/CNN_AdaBoost/blob/master/Jannesar_Shooshtari_Laitec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhgLZFSI6XvY",
        "colab_type": "code",
        "outputId": "42a3250d-1146-45c4-d8e8-9fda784ecaa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Import MNIST data\n",
        "mnist = input_data.read_data_sets(\"../1-dnn/\", one_hot=True)\n",
        "\n",
        "# Training Parameters\n",
        "num_steps = 200\n",
        "batch_size = 128\n",
        "display_step = 10\n",
        "strides = 1\n",
        "k = 2\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 784  # MNIST data input (img shape: 28*28)\n",
        "num_classes = 10  # MNIST total classes (0-9 digits)\n",
        "dropout = 0.5  # Dropout, probability to keep unit\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder('float', [None, num_input])\n",
        "Y = tf.placeholder('float',shape=[None,num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-af1da49a26b6>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../1-dnn/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ../1-dnn/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting ../1-dnn/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting ../1-dnn/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zhabes8A8UUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_cnn(x,dropout,num_classes):\n",
        "  x = tf.reshape(x, shape=[-1,28,28,1])\n",
        "\n",
        " \n",
        "  w_c_1 = tf.Variable(tf.random_normal([3, 3, 1, 32]))\n",
        "  w_c_2 = tf.Variable(tf.random_normal([3, 3, 32, 64]))\n",
        "  w_c_3 = tf.Variable(tf.random_normal([3, 3, 64, 128]))\n",
        "  b_c_1 = tf.Variable(tf.zeros([32]))\n",
        "  b_c_2 = tf.Variable(tf.zeros([64]))\n",
        "  b_c_3 = tf.Variable(tf.zeros([128]))\n",
        "  # The second three convolutional layer weights\n",
        "  w_c_4 = tf.Variable(tf.random_normal([3, 3, 128, 256]))\n",
        "  w_c_5 = tf.Variable(tf.random_normal([3, 3, 256, 512]))\n",
        "  w_c_6 = tf.Variable(tf.random_normal([3, 3, 512, 1024]))\n",
        "  b_c_4 = tf.Variable(tf.zeros([256]))\n",
        "  b_c_5 = tf.Variable(tf.zeros([512]))\n",
        "  b_c_6 = tf.Variable(tf.zeros([1024]))\n",
        "  # Fully connected weight\n",
        "  w_f_1 = tf.Variable(tf.random_normal([7*7*1024, 2048]))\n",
        "  w_f_2 = tf.Variable(tf.random_normal([2048, 1024]))\n",
        "  w_f_3 = tf.Variable(tf.random_normal([1024, 512]))\n",
        "  b_f_1 = tf.Variable(tf.zeros([2048]))\n",
        "  b_f_2 = tf.Variable(tf.zeros([1024]))\n",
        "  b_f_3 = tf.Variable(tf.zeros([512]))\n",
        "  \n",
        "  w_out = tf.Variable(tf.random_normal([512, num_classes]))\n",
        "  b_out = tf.Variable(tf.zeros([num_classes]))\n",
        "  \n",
        "  # first layer convolution\n",
        "  conv1 = tf.nn.bias_add(tf.nn.conv2d(x,w_c_1,name='C1',strides=[1,1,1,1],padding='SAME'),b_c_1)\n",
        "  conv1 = tf.nn.relu(conv1)\n",
        "  \n",
        "  # second layer convolution\n",
        "  conv2 = tf.nn.bias_add(tf.nn.conv2d(conv1,w_c_2,name='C2',strides=[1,1,1,1],padding='SAME'),b_c_2)\n",
        "  conv2 = tf.nn.relu(conv2)\n",
        "  \n",
        "  # third layer convolution\n",
        "  conv3 = tf.nn.bias_add(tf.nn.conv2d(conv2,w_c_3,name='C3',strides=[1,1,1,1],padding='SAME'),b_c_3)\n",
        "  conv3 = tf.nn.relu(conv3)\n",
        "  \n",
        "  # first Max Pooling (down-sampling)\n",
        "  pool_1 = tf.nn.max_pool2d(conv3,name='p1', ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
        "  \n",
        "  # fourth layer convolution\n",
        "  conv4 = tf.nn.bias_add(tf.nn.conv2d(pool_1,w_c_4,name='C4',strides=[1,1,1,1],padding='SAME'),b_c_4)\n",
        "  conv4 = tf.nn.relu(conv4)\n",
        "  \n",
        "  # fifth layer convolution\n",
        "  conv5 = tf.nn.bias_add(tf.nn.conv2d(conv4,w_c_5,name='C5',strides=[1,1,1,1],padding='SAME'),b_c_5)\n",
        "  conv5 = tf.nn.relu(conv5)\n",
        "  \n",
        "  # sixth layer convolution\n",
        "  conv6 = tf.nn.bias_add(tf.nn.conv2d(conv5,w_c_6,name='C6',strides=[1,1,1,1],padding='SAME'),b_c_6)\n",
        "  conv6 = tf.nn.relu(conv6)\n",
        "  \n",
        "  # second Max Pooling (down-sampling)\n",
        "  pool_2 = tf.nn.max_pool2d(conv6,name='p2',ksize=[1,2,2,1], strides=[1,2,2,1],padding='SAME')\n",
        "  \n",
        "  # first Fully connected layer\n",
        "  # Reshape conv6 output to fit fully connected layer input\n",
        "  # fc1 = tf.layers.dense(pool_2, 1024)\n",
        "  fc1 = tf.reshape(pool_2, [-1, w_f_1.get_shape().as_list()[0]])\n",
        "  fc1 = tf.add(tf.matmul(fc1, w_f_1), b_f_1)\n",
        "  fc1 = tf.nn.relu(fc1)\n",
        "  # Apply Dropout\n",
        "  fc1 = tf.nn.dropout(fc1, dropout)\n",
        "  \n",
        "  # second Fully connected layer\n",
        "  # fc2 = tf.layers.dense(fc1, 1024)\n",
        "  fc2 = tf.reshape(fc1, [-1, w_f_2.get_shape().as_list()[0]])\n",
        "  fc2 = tf.add(tf.matmul(fc2, w_f_2), b_f_2)\n",
        "  fc2 = tf.nn.relu(fc2)\n",
        "  # Third Fully connected layer\n",
        "  # fc3 = tf.layers.dense(fc2, 1024)\n",
        "  fc3 = tf.reshape(fc2, [-1, w_f_3.get_shape().as_list()[0]])\n",
        "  fc3 = tf.add(tf.matmul(fc3, w_f_3), b_f_3)\n",
        "  fc3 = tf.nn.relu(fc3)\n",
        "\n",
        "  out = tf.add(tf.matmul(fc3, w_out), b_out)\n",
        "  return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiXVAz7_EJp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = model_cnn(X, keep_prob,num_classes)\n",
        "prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCXAtJXEc1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "train_op = optimizer.minimize(loss_op)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WERUB2zzEkna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDf-TpOW6mF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "5d3875c5-85a6-4344-a3a5-e57ce0562448"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Please don't change these.\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "\n",
        "    for step in range(1, num_steps+1):\n",
        "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "        # batch_x = np.reshape(batch_x, (-1, 28, 28, 1))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 256 MNIST test images\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
        "                                      Y: mnist.test.labels[:256],\n",
        "                                      keep_prob: 1.0}))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 84147720683520.0000, Training Accuracy= 0.172\n",
            "Step 10, Minibatch Loss= 2316898926592.0000, Training Accuracy= 0.211\n",
            "Step 20, Minibatch Loss= 338190336000.0000, Training Accuracy= 0.133\n",
            "Step 30, Minibatch Loss= 41552891904.0000, Training Accuracy= 0.445\n",
            "Step 40, Minibatch Loss= 12233383936.0000, Training Accuracy= 0.570\n",
            "Step 50, Minibatch Loss= 7299719168.0000, Training Accuracy= 0.664\n",
            "Step 60, Minibatch Loss= 4685640704.0000, Training Accuracy= 0.789\n",
            "Step 70, Minibatch Loss= 4187026432.0000, Training Accuracy= 0.773\n",
            "Step 80, Minibatch Loss= 1413335808.0000, Training Accuracy= 0.914\n",
            "Step 90, Minibatch Loss= 2797360128.0000, Training Accuracy= 0.828\n",
            "Step 100, Minibatch Loss= 3197443584.0000, Training Accuracy= 0.859\n",
            "Step 110, Minibatch Loss= 2227255040.0000, Training Accuracy= 0.883\n",
            "Step 120, Minibatch Loss= 2875970816.0000, Training Accuracy= 0.859\n",
            "Step 130, Minibatch Loss= 1989464320.0000, Training Accuracy= 0.898\n",
            "Step 140, Minibatch Loss= 1524158720.0000, Training Accuracy= 0.906\n",
            "Step 150, Minibatch Loss= 2035196800.0000, Training Accuracy= 0.852\n",
            "Step 160, Minibatch Loss= 1219319296.0000, Training Accuracy= 0.922\n",
            "Step 170, Minibatch Loss= 1483073792.0000, Training Accuracy= 0.898\n",
            "Step 180, Minibatch Loss= 841768640.0000, Training Accuracy= 0.930\n",
            "Step 190, Minibatch Loss= 1308880896.0000, Training Accuracy= 0.891\n",
            "Step 200, Minibatch Loss= 1352391168.0000, Training Accuracy= 0.898\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.921875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSdD1-ivR9QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}