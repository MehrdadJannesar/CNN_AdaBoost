{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tesorflow_dAE",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadJannesar/CNN_AdaBoost/blob/master/Tesorflow_dAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ZctiZCcSW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "4d9ab760-24f4-435f-d256-aeba8f387307"
      },
      "source": [
        "import tensorflow as tf\n",
        "# from tensorflow.keras import datasets\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-c0b58d521865>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ./t10k-images-idx3-ubyte.gz\n",
            "Extracting ./t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhKhwXSAcW8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# x_train =  x_train / 255.\n",
        "# x_test =  x_train / 255.\n",
        "# x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "# x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "# noise_factor = 0.5\n",
        "# x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "# x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "\n",
        "x_train =  mnist.train.images / 255.\n",
        "x_test =   mnist.test.images / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "noise_factor = 0.5\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhAFjQpPcqqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 250\n",
        "batch_size = 100\n",
        "display_step = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "918OpS6ucsEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_input = 784\n",
        "n_hidden_1 = 256\n",
        "n_hidden_2 = 128\n",
        "n_hidden_3 = 64\n",
        "n_hidden_4 = 64\n",
        "n_hidden_5 = 128\n",
        "n_hidden_6 = 256\n",
        "n_out_put = 784"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNGcFWN0fzkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tf.placeholder(tf.float32, [None, 784], name='image')\n",
        "Y = tf.placeholder(tf.int32, [None, 10], name='label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihhzO3d-f1cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {\n",
        "    'h1': tf.get_variable(name='w1', shape=(784, 256), initializer=tf.random_normal_initializer()),\n",
        "    'h2': tf.get_variable(name='w2', shape=(256, 128), initializer=tf.random_normal_initializer()),\n",
        "    'h3': tf.get_variable(name='w3', shape=(128, 64), initializer=tf.random_normal_initializer()),\n",
        "    'h4': tf.get_variable(name='w4', shape=(64, 64), initializer=tf.random_normal_initializer()),\n",
        "    'h5': tf.get_variable(name='w5', shape=(64, 128), initializer=tf.random_normal_initializer()),\n",
        "    'h6': tf.get_variable(name='w6', shape=(128, 256), initializer=tf.random_normal_initializer()),\n",
        "    'h7': tf.get_variable(name='w7', shape=(256, 784), initializer=tf.random_normal_initializer()),\n",
        "    'out': tf.get_variable(name='w_out', shape=(784, 10), initializer=tf.random_normal_initializer())\n",
        "\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZluVTFwgIfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "biases = {\n",
        "    'b1': tf.get_variable(name='b1', shape=(1, 256), initializer=tf.zeros_initializer()),\n",
        "    'b2': tf.get_variable(name='b2', shape=(1, 128), initializer=tf.zeros_initializer()),\n",
        "    'b3': tf.get_variable(name='b3', shape=(1, 64), initializer=tf.zeros_initializer()),\n",
        "    'b4': tf.get_variable(name='b4', shape=(1, 64), initializer=tf.zeros_initializer()),\n",
        "    'b5': tf.get_variable(name='b5', shape=(1, 128), initializer=tf.zeros_initializer()),\n",
        "    'b6': tf.get_variable(name='b6', shape=(1, 256), initializer=tf.zeros_initializer()),\n",
        "    'b7': tf.get_variable(name='b7', shape=(1, 784), initializer=tf.zeros_initializer()),\n",
        "    'out': tf.get_variable(name='b_out', shape=(1, 10), initializer=tf.zeros_initializer())    \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaKAYFnmgVKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def d_Autoencoder(x):\n",
        "  layer_1 = tf.matmul(x, weights['h1']) + biases['b1']\n",
        "  layer_1 = tf.nn.relu(layer_1)\n",
        "  \n",
        "  layer_2 = tf.matmul(layer_1, weights['h2']) + biases['b2']\n",
        "  layer_2 = tf.nn.relu(layer_2)\n",
        "  \n",
        "  layer_3 = tf.matmul(layer_2, weights['h3']) + biases['b3']\n",
        "  layer_3 = tf.nn.relu(layer_3)\n",
        "  \n",
        "  layer_4 = tf.matmul(layer_3, weights['h4']) + biases['b4']\n",
        "  layer_4 = tf.nn.relu(layer_4)\n",
        "  \n",
        "  layer_5 = tf.matmul(layer_4, weights['h5']) + biases['b5']\n",
        "  layer_5 = tf.nn.relu(layer_5)\n",
        "  \n",
        "  layer_6 = tf.matmul(layer_5, weights['h6']) + biases['b6']\n",
        "  layer_6 = tf.nn.relu(layer_6)\n",
        "  \n",
        "  layer_7 = tf.matmul(layer_6, weights['h7']) + biases['b7']\n",
        "  layer_7 = tf.nn.relu(layer_7)\n",
        "  \n",
        "  out_layer = tf.matmul(layer_7, weights['out']) + biases['out']\n",
        "  return out_layer\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRfY4IRlhNw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logits = d_Autoencoder(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WciGHwWJhQ5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "351aaf78-d0e7-40c5-f2d7-d1c31754c61f"
      },
      "source": [
        "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
        "loss_op = tf.reduce_mean(entropy)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_op)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-10-efdd7f8be963>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5zQQZfBhWtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9727441e-34ca-4821-fbbe-3400ca8e7a5f"
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "config = tf.ConfigProto()\n",
        "\n",
        "with tf.Session(config=config) as sess:\n",
        "    # Initializing the session\n",
        "    sess.run(init)\n",
        "                   \n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(mnist.train.num_examples / batch_size)\n",
        "        # Loop over all batches\n",
        "        for i in range(total_batch):\n",
        "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, c = sess.run([optimizer, loss_op], {X: batch_x, Y: batch_y})\n",
        "\n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "        # Display logs per epoch step\n",
        "        if epoch % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch + 1), \"cost={:.9f}\".format(avg_cost))\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    pred = tf.nn.softmax(logits)\n",
        "    correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y, 1))\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    print('')\n",
        "    print(\"Test Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost=3599481.645113636\n",
            "Epoch: 0002 cost=729907.715852272\n",
            "Epoch: 0003 cost=419679.221122159\n",
            "Epoch: 0004 cost=282634.751605114\n",
            "Epoch: 0005 cost=204597.016946023\n",
            "Epoch: 0006 cost=152868.189232955\n",
            "Epoch: 0007 cost=123780.311516335\n",
            "Epoch: 0008 cost=100572.539726563\n",
            "Epoch: 0009 cost=81266.641395597\n",
            "Epoch: 0010 cost=68468.115422585\n",
            "Epoch: 0011 cost=59352.261209162\n",
            "Epoch: 0012 cost=49831.254030873\n",
            "Epoch: 0013 cost=41608.577644085\n",
            "Epoch: 0014 cost=38587.393007258\n",
            "Epoch: 0015 cost=32604.441764360\n",
            "Epoch: 0016 cost=29955.665845316\n",
            "Epoch: 0017 cost=27618.147455500\n",
            "Epoch: 0018 cost=25005.631247170\n",
            "Epoch: 0019 cost=22475.577971816\n",
            "Epoch: 0020 cost=21081.611987679\n",
            "Epoch: 0021 cost=18046.058232144\n",
            "Epoch: 0022 cost=17523.851856440\n",
            "Epoch: 0023 cost=17182.199914853\n",
            "Epoch: 0024 cost=13704.966824533\n",
            "Epoch: 0025 cost=13786.075817413\n",
            "Epoch: 0026 cost=12988.057622958\n",
            "Epoch: 0027 cost=11732.666492802\n",
            "Epoch: 0028 cost=10256.617896732\n",
            "Epoch: 0029 cost=11310.416465815\n",
            "Epoch: 0030 cost=10054.246000137\n",
            "Epoch: 0031 cost=9902.716775818\n",
            "Epoch: 0032 cost=9630.902926954\n",
            "Epoch: 0033 cost=7465.943859482\n",
            "Epoch: 0034 cost=9368.639957137\n",
            "Epoch: 0035 cost=9653.310654352\n",
            "Epoch: 0036 cost=8137.465464054\n",
            "Epoch: 0037 cost=7968.938826765\n",
            "Epoch: 0038 cost=7581.326508071\n",
            "Epoch: 0039 cost=6743.074674875\n",
            "Epoch: 0040 cost=6930.168895583\n",
            "Epoch: 0041 cost=6702.513547148\n",
            "Epoch: 0042 cost=6147.756419244\n",
            "Epoch: 0043 cost=6401.803125950\n",
            "Epoch: 0044 cost=5826.969178551\n",
            "Epoch: 0045 cost=4885.371507589\n",
            "Epoch: 0046 cost=6334.670781399\n",
            "Epoch: 0047 cost=4811.149802048\n",
            "Epoch: 0048 cost=5032.180444655\n",
            "Epoch: 0049 cost=5049.416246638\n",
            "Epoch: 0050 cost=5031.457602400\n",
            "Epoch: 0051 cost=4075.263715081\n",
            "Epoch: 0052 cost=5405.262938704\n",
            "Epoch: 0053 cost=5616.712301771\n",
            "Epoch: 0054 cost=3864.100102147\n",
            "Epoch: 0055 cost=3374.858490486\n",
            "Epoch: 0056 cost=4359.290767026\n",
            "Epoch: 0057 cost=5092.979815712\n",
            "Epoch: 0058 cost=3818.509478572\n",
            "Epoch: 0059 cost=3596.200990680\n",
            "Epoch: 0060 cost=3982.396996817\n",
            "Epoch: 0061 cost=3771.874696440\n",
            "Epoch: 0062 cost=3747.182695954\n",
            "Epoch: 0063 cost=3284.591820138\n",
            "Epoch: 0064 cost=3403.535342441\n",
            "Epoch: 0065 cost=3687.840582696\n",
            "Epoch: 0066 cost=3000.158011024\n",
            "Epoch: 0067 cost=3769.413742287\n",
            "Epoch: 0068 cost=3286.328943018\n",
            "Epoch: 0069 cost=3025.881126532\n",
            "Epoch: 0070 cost=3736.359769208\n",
            "Epoch: 0071 cost=3242.720205799\n",
            "Epoch: 0072 cost=3424.405181302\n",
            "Epoch: 0073 cost=3193.434582769\n",
            "Epoch: 0074 cost=3522.050371652\n",
            "Epoch: 0075 cost=2749.227046656\n",
            "Epoch: 0076 cost=3260.418665398\n",
            "Epoch: 0077 cost=2320.784722349\n",
            "Epoch: 0078 cost=3079.274906991\n",
            "Epoch: 0079 cost=2618.511722209\n",
            "Epoch: 0080 cost=2680.246305129\n",
            "Epoch: 0081 cost=3160.675957372\n",
            "Epoch: 0082 cost=2502.985923445\n",
            "Epoch: 0083 cost=2315.052075016\n",
            "Epoch: 0084 cost=2015.424840357\n",
            "Epoch: 0085 cost=2787.573194486\n",
            "Epoch: 0086 cost=2651.208963059\n",
            "Epoch: 0087 cost=1811.383843058\n",
            "Epoch: 0088 cost=2713.143088816\n",
            "Epoch: 0089 cost=2478.095954595\n",
            "Epoch: 0090 cost=2359.591073501\n",
            "Epoch: 0091 cost=2684.986746061\n",
            "Epoch: 0092 cost=2556.027768478\n",
            "Epoch: 0093 cost=2870.284933707\n",
            "Epoch: 0094 cost=2665.621918474\n",
            "Epoch: 0095 cost=1984.024143729\n",
            "Epoch: 0096 cost=1730.314258206\n",
            "Epoch: 0097 cost=2053.841084879\n",
            "Epoch: 0098 cost=2912.124341306\n",
            "Epoch: 0099 cost=1775.743232276\n",
            "Epoch: 0100 cost=2004.311677685\n",
            "Epoch: 0101 cost=1861.644614436\n",
            "Epoch: 0102 cost=1642.498530364\n",
            "Epoch: 0103 cost=1697.231579729\n",
            "Epoch: 0104 cost=2645.907439471\n",
            "Epoch: 0105 cost=2618.448105505\n",
            "Epoch: 0106 cost=1701.089518377\n",
            "Epoch: 0107 cost=1398.661689089\n",
            "Epoch: 0108 cost=2256.185874497\n",
            "Epoch: 0109 cost=2158.619503500\n",
            "Epoch: 0110 cost=1138.119011591\n",
            "Epoch: 0111 cost=2248.610343033\n",
            "Epoch: 0112 cost=1571.941015556\n",
            "Epoch: 0113 cost=1687.925335652\n",
            "Epoch: 0114 cost=1930.287424060\n",
            "Epoch: 0115 cost=1668.752870405\n",
            "Epoch: 0116 cost=2215.512533791\n",
            "Epoch: 0117 cost=1335.078402765\n",
            "Epoch: 0118 cost=1678.823061732\n",
            "Epoch: 0119 cost=2413.642337997\n",
            "Epoch: 0120 cost=1275.467258651\n",
            "Epoch: 0121 cost=1491.579183377\n",
            "Epoch: 0122 cost=1648.324396984\n",
            "Epoch: 0123 cost=1590.403867689\n",
            "Epoch: 0124 cost=1263.521737102\n",
            "Epoch: 0125 cost=1617.423146418\n",
            "Epoch: 0126 cost=1538.161000411\n",
            "Epoch: 0127 cost=1738.970659409\n",
            "Epoch: 0128 cost=1631.507754791\n",
            "Epoch: 0129 cost=1554.481977608\n",
            "Epoch: 0130 cost=1300.181457034\n",
            "Epoch: 0131 cost=1327.300323070\n",
            "Epoch: 0132 cost=1777.905569070\n",
            "Epoch: 0133 cost=1470.394917679\n",
            "Epoch: 0134 cost=1248.566670213\n",
            "Epoch: 0135 cost=1514.170111299\n",
            "Epoch: 0136 cost=1568.151247711\n",
            "Epoch: 0137 cost=2045.998828977\n",
            "Epoch: 0138 cost=1119.782263017\n",
            "Epoch: 0139 cost=1096.305975009\n",
            "Epoch: 0140 cost=1526.040219279\n",
            "Epoch: 0141 cost=897.133411137\n",
            "Epoch: 0142 cost=1870.185120070\n",
            "Epoch: 0143 cost=1469.526399134\n",
            "Epoch: 0144 cost=692.277709056\n",
            "Epoch: 0145 cost=1142.868639273\n",
            "Epoch: 0146 cost=1155.475913530\n",
            "Epoch: 0147 cost=2070.034721298\n",
            "Epoch: 0148 cost=1774.418646854\n",
            "Epoch: 0149 cost=1348.379839138\n",
            "Epoch: 0150 cost=1019.758885193\n",
            "Epoch: 0151 cost=1345.120885773\n",
            "Epoch: 0152 cost=1682.101265703\n",
            "Epoch: 0153 cost=1325.489326529\n",
            "Epoch: 0154 cost=779.646571680\n",
            "Epoch: 0155 cost=1164.674865785\n",
            "Epoch: 0156 cost=883.193232214\n",
            "Epoch: 0157 cost=1645.936902098\n",
            "Epoch: 0158 cost=1156.616434467\n",
            "Epoch: 0159 cost=1559.960655633\n",
            "Epoch: 0160 cost=1051.290769086\n",
            "Epoch: 0161 cost=890.420999187\n",
            "Epoch: 0162 cost=1119.484544504\n",
            "Epoch: 0163 cost=1526.860911005\n",
            "Epoch: 0164 cost=1106.534471762\n",
            "Epoch: 0165 cost=1200.797643433\n",
            "Epoch: 0166 cost=952.392171603\n",
            "Epoch: 0167 cost=900.847771606\n",
            "Epoch: 0168 cost=1095.625442814\n",
            "Epoch: 0169 cost=1285.275013109\n",
            "Epoch: 0170 cost=933.580862416\n",
            "Epoch: 0171 cost=749.415977986\n",
            "Epoch: 0172 cost=1257.204987640\n",
            "Epoch: 0173 cost=1071.696849930\n",
            "Epoch: 0174 cost=966.973106814\n",
            "Epoch: 0175 cost=703.217249326\n",
            "Epoch: 0176 cost=1247.340173194\n",
            "Epoch: 0177 cost=591.378343263\n",
            "Epoch: 0178 cost=832.137018182\n",
            "Epoch: 0179 cost=1082.722925651\n",
            "Epoch: 0180 cost=955.557590429\n",
            "Epoch: 0181 cost=1004.727820830\n",
            "Epoch: 0182 cost=953.674136970\n",
            "Epoch: 0183 cost=683.671696833\n",
            "Epoch: 0184 cost=1210.581336040\n",
            "Epoch: 0185 cost=1107.486557624\n",
            "Epoch: 0186 cost=731.040294401\n",
            "Epoch: 0187 cost=460.782168657\n",
            "Epoch: 0188 cost=1121.621938787\n",
            "Epoch: 0189 cost=986.941435673\n",
            "Epoch: 0190 cost=579.233059845\n",
            "Epoch: 0191 cost=782.871975311\n",
            "Epoch: 0192 cost=790.130390757\n",
            "Epoch: 0193 cost=1262.685853826\n",
            "Epoch: 0194 cost=1050.260275511\n",
            "Epoch: 0195 cost=1085.645982534\n",
            "Epoch: 0196 cost=690.631731701\n",
            "Epoch: 0197 cost=876.860511157\n",
            "Epoch: 0198 cost=768.406301838\n",
            "Epoch: 0199 cost=964.764265789\n",
            "Epoch: 0200 cost=559.924434905\n",
            "Epoch: 0201 cost=1047.253439230\n",
            "Epoch: 0202 cost=961.496011814\n",
            "Epoch: 0203 cost=522.528406018\n",
            "Epoch: 0204 cost=537.713885682\n",
            "Epoch: 0205 cost=1052.081924452\n",
            "Epoch: 0206 cost=483.776515775\n",
            "Epoch: 0207 cost=889.346415294\n",
            "Epoch: 0208 cost=651.578490098\n",
            "Epoch: 0209 cost=855.558206348\n",
            "Epoch: 0210 cost=885.887443265\n",
            "Epoch: 0211 cost=705.796818121\n",
            "Epoch: 0212 cost=495.020374097\n",
            "Epoch: 0213 cost=949.182829742\n",
            "Epoch: 0214 cost=701.614696295\n",
            "Epoch: 0215 cost=777.645884448\n",
            "Epoch: 0216 cost=1037.907930365\n",
            "Epoch: 0217 cost=470.188829242\n",
            "Epoch: 0218 cost=481.067982871\n",
            "Epoch: 0219 cost=1105.535426067\n",
            "Epoch: 0220 cost=612.976338269\n",
            "Epoch: 0221 cost=529.669665722\n",
            "Epoch: 0222 cost=811.235530496\n",
            "Epoch: 0223 cost=488.181788885\n",
            "Epoch: 0224 cost=419.360367644\n",
            "Epoch: 0225 cost=777.417671502\n",
            "Epoch: 0226 cost=1049.978634186\n",
            "Epoch: 0227 cost=737.401793193\n",
            "Epoch: 0228 cost=730.784439926\n",
            "Epoch: 0229 cost=354.925647091\n",
            "Epoch: 0230 cost=592.318304616\n",
            "Epoch: 0231 cost=696.290729377\n",
            "Epoch: 0232 cost=814.175318916\n",
            "Epoch: 0233 cost=504.077623666\n",
            "Epoch: 0234 cost=594.169690448\n",
            "Epoch: 0235 cost=626.306015098\n",
            "Epoch: 0236 cost=569.733592862\n",
            "Epoch: 0237 cost=556.496394105\n",
            "Epoch: 0238 cost=484.219306211\n",
            "Epoch: 0239 cost=640.627130299\n",
            "Epoch: 0240 cost=698.262021741\n",
            "Epoch: 0241 cost=532.587604048\n",
            "Epoch: 0242 cost=440.427963888\n",
            "Epoch: 0243 cost=587.985296550\n",
            "Epoch: 0244 cost=372.794438336\n",
            "Epoch: 0245 cost=845.489506725\n",
            "Epoch: 0246 cost=473.996586942\n",
            "Epoch: 0247 cost=556.144172578\n",
            "Epoch: 0248 cost=646.670093575\n",
            "Epoch: 0249 cost=982.211330768\n",
            "Epoch: 0250 cost=755.864282332\n",
            "Optimization Finished!\n",
            "\n",
            "Test Accuracy: 0.966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdQ8v6uMwDtv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}